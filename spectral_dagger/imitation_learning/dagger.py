import numpy as np
from spectral_dagger.mdp import UniformRandomPolicy, MDPPolicy
from spectral_dagger.pomdp import BeliefStatePolicy
from spectral_dagger import LearningAlgorithm


def dagger(
        mdp, expert, learning_alg, beta,
        n_iterations, n_samples_per_iter, horizon,
        initial_policy=None):
    """ Fully observable DAgger.

    Operates on an MDP and learns policies that operate on states of that MDP.

    """
    print "Running DAgger..."

    if initial_policy is None:
        initial_policy = UniformRandomPolicy(mdp.actions)

    policy = initial_policy

    policies = [initial_policy]

    states = []
    expert_actions = []

    for i in range(n_iterations):
        b = beta.next()

        print "Starting DAgger iteration ", i
        print "Beta for this iteration is ", b

        for j in range(n_samples_per_iter):
            mdp.reset()

            state = mdp.current_state

            expert.reset(state)
            policy.reset(state)

            for t in range(horizon):
                states.append(state)

                expert_action = expert.get_action()
                expert_actions.append(expert_action)

                if np.random.random() < b:
                    action = expert_action
                else:
                    action = policy.get_action()

                next_state, _ = mdp.update(action)

                policy.update(next_state, action)
                expert.update(next_state, action)

                state = next_state

        policy = learning_alg.fit(mdp, states, expert_actions)

        policies.append(policy)

    print "Done DAgger."

    return policies


def po_dagger(
        pomdp, expert, learning_alg, beta,
        n_iterations, n_samples_per_iter, horizon,
        initial_policy=None):
    """ Partially Observable DAgger.

    Operates on a POMDP and learns policies that operate on
    action-observation histories generated by the POMDP.

    """
    print "Running PO-DAgger..."

    if initial_policy is None:
        initial_policy = UniformRandomPolicy(pomdp.actions)

    policy = initial_policy

    policies = [initial_policy]

    trajectories = []
    expert_actions = []

    for i in range(n_iterations):
        b = beta.next()

        print "Starting PO-DAgger iteration ", i
        print "Beta for this iteration is ", b

        for j in range(n_samples_per_iter):
            pomdp.reset()
            expert.reset()
            policy.reset()

            trajectories.append([])
            expert_actions.append([])

            for t in range(horizon):
                expert_action = expert.get_action()
                expert_actions[-1].append(expert_action)

                if np.random.random() < b:
                    action = expert_action
                else:
                    action = policy.get_action()

                obs, _ = pomdp.update(action)

                policy.update(obs, action)
                expert.update(obs, action)

                trajectories[-1].append((action, obs))

        policy = learning_alg.fit(
            pomdp, trajectories, expert_actions)

        policies.append(policy)

    print "Done PO-DAgger."

    return policies


class StateClassifier(LearningAlgorithm):
    """ Learn a policy that classifies (state, action) pairs.

    Learns a policy from state-action pairs, where the actions are supplied by
    an expert. Uses a predictor to try to reproduce the mapping.

    Parameters
    ----------
    predictor: any
        Must implement the sklearn Estimator and Predictor interfaces.
        http://scikit-learn.org/stable/developers/contributing.html

    feature_extractor: FeatureExtractor instance
        Turns the state into a feature vector.

    """
    def __init__(self, predictor, feature_extractor):
        self.predictor = predictor
        self.feature_extractor = feature_extractor

    def fit(self, mdp, states, actions):
        state_vectors = np.array(
            [self.feature_extractor.as_vector(state) for state in states])

        # Most sklearn predictors operate on strings or numbers
        action_lookup = {str(a): a for a in set(actions)}
        str_actions = [str(a) for a in actions]

        self.predictor.fit(state_vectors, str_actions)

        def f(state):
            features = self.feature_extractor.as_vector(state)
            features = features.reshape(1, -1)
            action_string = self.predictor.predict(features)[0]
            return action_lookup[action_string]

        policy = MDPPolicy(mdp, f)
        policy.predictor = self.predictor
        policy.feature_extractor = self.feature_extractor

        return policy


class BeliefStateClassifier(LearningAlgorithm):
    """ Learn a policy that classifies (belief-state, action) pairs.

    Learns a policy from pairs of the form (b, a) where b is a belief state and
    a is the action taken in reponse to that belief state by an expert. Uses a
    classifier to try to reproduce the mapping.

    Parameters
    ----------
    predictor: any
        Must implement the sklearn Estimator and Predictor interfaces.
        http://scikit-learn.org/stable/developers/contributing.html

    """
    def __init__(self, predictor):
        self.predictor = predictor

    def fit(self, pomdp, trajectories, actions):
        """ Returns a policy trained on the given data.

        Parameters
        ----------
        pomdp: POMDP
            The pomdp that the data was generated from.

        trajectories: list of lists of action-observation pairs
            Each sublist corresponds to a trajectory.

        actions: list of lists of actions
            Each sublist contains the actions generated by the expert in
            response to trajectories. The the j-th entry of the i-th sublist
            gives the action chosen by the expert in response to the first
            j-1 action-observation pairs in the i-th trajectory in
            `trajectories`. The actions in this data structure are not
            necessarily the same as the actions in `trajectories`.

        """
        policy = BeliefStatePolicy(pomdp)

        belief_states = []
        flat_actions = []

        # Turn the trajectories into belief states, and flatten out the
        # actions so that we have our data set in the (X, Y) format required
        # by sklearn estimators.
        for t, response_actions in zip(trajectories, actions):
            policy.reset()

            for (a, o), response_action in zip(t, response_actions):
                belief_states.append(policy.belief_state)
                flat_actions.append(response_action)

                policy.update(o, a)

        # Most sklearn predictors operate on strings or numbers
        action_lookup = {str(a): a for a in set(flat_actions)}
        str_actions = [str(a) for a in flat_actions]

        self.predictor.fit(belief_states, str_actions)

        def f(belief_state):
            belief_state = belief_state.reshape(1, -1)
            action_string = self.predictor.predict(belief_state)[0]
            return action_lookup[action_string]

        return BeliefStatePolicy(pomdp, f)
